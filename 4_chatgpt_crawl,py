import requests
from bs4 import BeautifulSoup

# 기사 제목을 저장할 빈 리스트를 초기화합니다.
article_titles = []

# 페이지 1부터 10까지 루프를 돕니다. 
for page in range(1, 11):
    # 크롤링할 웹 페이지의 URL을 생성합니다.
    url = f"https://search.naver.com/search.naver?where=news&sm=tab_jum&query=테슬라&start={(page - 1) * 10 + 1}"

    # 웹페이지 내용을 가져옵니다. 
    response = requests.get(url)
    html_content = response.text

    # HTML을 파싱하기 위해 BeautifulSoup 객체를 생성합니다. 
    soup = BeautifulSoup(html_content, "html.parser")

    # 'news_tit' 클래스를 가진 모든 요소를 찾습니다.
    article_elements = soup.find_all("a", class_="news_tit")

    # 기사 제목을 추출하고 리스트에 append합니다.
    for article in article_elements:
        title = article.get_text(strip=True)  # 텍스트를 가져오고 불필요한 공백을 제거합니다
        article_titles.append(title)

# 기사 제목 리스트를 하나의 문자열로 변환합니다.
titles_text = "\n".join(article_titles)

# 기사 제목을 텍스트 파일로 저장합니다.
search_term = "테슬라"
output_filename = f"{search_term}_기사제목.txt"

with open(output_filename, "w", encoding="utf-8") as f:
    f.write(titles_text)

# 성공적으로 추출 및 저장되었음을 메시지로 출력한다.
print(f"Article titles extracted and saved to '{output_filename}'.")